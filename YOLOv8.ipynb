{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "  <a href=\"https://ultralytics.com/yolov8\" target=\"_blank\">\n",
        "    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\n",
        "\n",
        "  [‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://docs.ultralytics.com/hi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)\n",
        "\n",
        "  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n",
        "  <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "  <a href=\"https://www.kaggle.com/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
        "\n",
        "Welcome to the Ultralytics YOLOv8 üöÄ notebook! <a href=\"https://github.com/ultralytics/ultralytics\">YOLOv8</a> is the latest version of the YOLO (You Only Look Once) AI models developed by <a href=\"https://ultralytics.com\">Ultralytics</a>. This notebook serves as the starting point for exploring the various resources available to help you get started with YOLOv8 and understand its features and capabilities.\n",
        "\n",
        "YOLOv8 models are fast, accurate, and easy to use, making them ideal for various object detection and image segmentation tasks. They can be trained on large datasets and run on diverse hardware platforms, from CPUs to GPUs.\n",
        "\n",
        "We hope that the resources in this notebook will help you get the most out of YOLOv8. Please browse the YOLOv8 <a href=\"https://docs.ultralytics.com/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/ultralytics\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://pepy.tech/project/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.78  Python-3.8.10 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3060, 12287MiB)\n",
            "Setup complete  (4 CPUs, 16.0 GB RAM, 16.8/251.8 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Predict\n",
        "\n",
        "YOLOv8 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLOv8 Predict Docs](https://docs.ultralytics.com/modes/train/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9ZbuQCH7FX",
        "outputId": "84f32db2-80b0-4f35-9a2a-a56d11f7863f"
      },
      "outputs": [],
      "source": [
        "# Run inference on an image with YOLOv8n\n",
        "!yolo predict model=yolov8s.pt source='./test_video.mp4' classes=[2,3,7]  device = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Val\n",
        "Validate a model's accuracy on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset's `val` or `test` splits. The latest YOLOv8 [models](https://github.com/ultralytics/ultralytics#models) are downloaded automatically the first time they are used. See [YOLOv8 Val Docs](https://docs.ultralytics.com/modes/val/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQPtK1QYVaD_"
      },
      "outputs": [],
      "source": [
        "# Download COCO val\n",
        "import torch\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
        "!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58w8JLpMnjH",
        "outputId": "bed10d45-ceb6-4b6f-86b7-9428208b142a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.3 üöÄ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "Dataset 'coco8.yaml' images not found ‚ö†Ô∏è, missing path '/content/datasets/coco8/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip'...\n",
            "100% 433k/433k [00:00<00:00, 14.2MB/s]\n",
            "Unzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100% 25/25 [00:00<00:00, 1093.93file/s]\n",
            "Dataset download success ‚úÖ (1.3s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 17.4MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100% 4/4 [00:00<00:00, 157.00it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8/labels/val.cache\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:06<00:00,  6.89s/it]\n",
            "                   all          4         17      0.621      0.833      0.888       0.63\n",
            "                person          4         10      0.721        0.5      0.519      0.269\n",
            "                   dog          4          1       0.37          1      0.995      0.597\n",
            "                 horse          4          2      0.751          1      0.995      0.631\n",
            "              elephant          4          2      0.505        0.5      0.828      0.394\n",
            "              umbrella          4          1      0.564          1      0.995      0.995\n",
            "          potted plant          4          1      0.814          1      0.995      0.895\n",
            "Speed: 0.3ms preprocess, 4.9ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        }
      ],
      "source": [
        "# Validate YOLOv8n on COCO8 val\n",
        "!yolo val model=yolov8n.pt data=coco8.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://ultralytics.com/hub\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"/></a></p>\n",
        "\n",
        "Train YOLOv8 on [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/) datasets. See [YOLOv8 Train Docs](https://docs.ultralytics.com/modes/train/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktegpM42AooT"
      },
      "outputs": [],
      "source": [
        "#@title Select YOLOv8 üöÄ logger {run: 'auto'}\n",
        "logger = 'Comet' #@param ['Comet', 'TensorBoard']\n",
        "\n",
        "if logger == 'Comet':\n",
        "  %pip install -q comet_ml\n",
        "  import comet_ml; comet_ml.init()\n",
        "elif logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "9f60c6cb-fa9c-4785-cb7a-71d40abeaf38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.3 üöÄ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100% 4/4 [00:00<00:00, 837.19it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco8/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100% 4/4 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3      0.81G      1.039      3.146      1.498         25        640: 100% 1/1 [00:01<00:00,  1.51s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.32it/s]\n",
            "                   all          4         17       0.62      0.885      0.888      0.621\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3     0.772G      1.169      2.779      1.442         36        640: 100% 1/1 [00:00<00:00,  8.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.22it/s]\n",
            "                   all          4         17      0.595      0.903      0.888      0.616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3     0.776G     0.6701      3.697      1.096         17        640: 100% 1/1 [00:00<00:00,  6.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  5.66it/s]\n",
            "                   all          4         17      0.577      0.833      0.874      0.614\n",
            "\n",
            "3 epochs completed in 0.002 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.3 üöÄ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 18.23it/s]\n",
            "                   all          4         17      0.617      0.884      0.888      0.622\n",
            "                person          4         10       0.67        0.5       0.52      0.278\n",
            "                   dog          4          1      0.361          1      0.995      0.597\n",
            "                 horse          4          2      0.728          1      0.995      0.631\n",
            "              elephant          4          2      0.602      0.805      0.828      0.332\n",
            "              umbrella          4          1      0.553          1      0.995      0.995\n",
            "          potted plant          4          1      0.789          1      0.995      0.895\n",
            "Speed: 0.3ms preprocess, 4.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "# Train YOLOv8n on COCO8 for 3 epochs\n",
        "!yolo train model=yolov8n.pt data=coco8.yaml epochs=3 imgsz=640\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPZZeNrLCQG6"
      },
      "source": [
        "# 4. Export\n",
        "\n",
        "Export a YOLOv8 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLOv8 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
        "\n",
        "- üí° ProTip: Export to [ONNX](https://docs.ultralytics.com/integrations/onnx/) or [OpenVINO](https://docs.ultralytics.com/integrations/openvino/) for up to 3x CPU speedup.  \n",
        "- üí° ProTip: Export to [TensorRT](https://docs.ultralytics.com/integrations/tensorrt/) for up to 5x GPU speedup.\n",
        "\n",
        "| Format                                                                   | `format` Argument | Model                     | Metadata | Arguments                                                            |\n",
        "|--------------------------------------------------------------------------|-------------------|---------------------------|----------|----------------------------------------------------------------------|\n",
        "| [PyTorch](https://pytorch.org/)                                          | -                 | `yolov8n.pt`              | ‚úÖ        | -                                                                    |\n",
        "| [TorchScript](https://docs.ultralytics.com/integrations/torchscript)     | `torchscript`     | `yolov8n.torchscript`     | ‚úÖ        | `imgsz`, `optimize`, `batch`                                         |\n",
        "| [ONNX](https://docs.ultralytics.com/integrations/onnx)                   | `onnx`            | `yolov8n.onnx`            | ‚úÖ        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`, `batch`             |\n",
        "| [OpenVINO](https://docs.ultralytics.com/integrations/openvino)           | `openvino`        | `yolov8n_openvino_model/` | ‚úÖ        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [TensorRT](https://docs.ultralytics.com/integrations/tensorrt)           | `engine`          | `yolov8n.engine`          | ‚úÖ        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace`, `int8`, `batch` |\n",
        "| [CoreML](https://docs.ultralytics.com/integrations/coreml)               | `coreml`          | `yolov8n.mlpackage`       | ‚úÖ        | `imgsz`, `half`, `int8`, `nms`, `batch`                              |\n",
        "| [TF SavedModel](https://docs.ultralytics.com/integrations/tf-savedmodel) | `saved_model`     | `yolov8n_saved_model/`    | ‚úÖ        | `imgsz`, `keras`, `int8`, `batch`                                    |\n",
        "| [TF GraphDef](https://docs.ultralytics.com/integrations/tf-graphdef)     | `pb`              | `yolov8n.pb`              | ‚ùå        | `imgsz`, `batch`                                                     |\n",
        "| [TF Lite](https://docs.ultralytics.com/integrations/tflite)              | `tflite`          | `yolov8n.tflite`          | ‚úÖ        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [TF Edge TPU](https://docs.ultralytics.com/integrations/edge-tpu)        | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ‚úÖ        | `imgsz`, `batch`                                                     |\n",
        "| [TF.js](https://docs.ultralytics.com/integrations/tfjs)                  | `tfjs`            | `yolov8n_web_model/`      | ‚úÖ        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [PaddlePaddle](https://docs.ultralytics.com/integrations/paddlepaddle)   | `paddle`          | `yolov8n_paddle_model/`   | ‚úÖ        | `imgsz`, `batch`                                                     |\n",
        "| [NCNN](https://docs.ultralytics.com/integrations/ncnn)                   | `ncnn`            | `yolov8n_ncnn_model/`     | ‚úÖ        | `imgsz`, `half`, `batch`                                             |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIjW4igCjqD",
        "outputId": "947e65cc-79c8-4713-bfd4-3139903ac05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.3 üöÄ Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Xeon 2.00GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.2.1+cu121...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ‚úÖ 2.0s, saved as 'yolov8n.torchscript' (12.4 MB)\n",
            "\n",
            "Export complete (4.0s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov8n.torchscript imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolov8n.torchscript imgsz=640 data=coco.yaml  \n",
            "Visualize:       https://netron.app\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ],
      "source": [
        "!yolo export model=yolov8n.pt format=torchscript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      },
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLOv8 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLOv8 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLOv8 Python Docs](https://docs.ultralytics.com/usage/python/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpF9-vS_DAaf"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "# results = model.train(data='coco8.yaml', epochs=3)  # train the model\n",
        "# results = model.val()  # evaluate model performance on the validation set\n",
        "# results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
        "# results = model.export(format='onnx')  # export the model to ONNX format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phm9ccmOKye5"
      },
      "source": [
        "# 6. Tasks\n",
        "\n",
        "YOLOv8 can train, val, predict and export models for the most common tasks in vision AI: [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/). See [YOLOv8 Tasks Docs](https://docs.ultralytics.com/tasks/) for more information.\n",
        "\n",
        "<br><img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq26lwpYK1lq"
      },
      "source": [
        "## 1. Detection\n",
        "\n",
        "YOLOv8 _detection_ models have no suffix and are the default YOLOv8 models, i.e. `yolov8n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8Go5qqS9LbC5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.2.79 available  Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.2.78  Python-3.8.10 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3060, 12287MiB)\n",
            "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
            "\n",
            "Dataset 'coco8.yaml' images not found , missing path 'D:\\HighwayCounter_YOLOv8\\datasets\\coco8\\images\\val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to 'D:\\HighwayCounter_YOLOv8\\datasets\\coco8.zip'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 433k/433k [00:00<00:00, 586kB/s]\n",
            "Unzipping D:\\HighwayCounter_YOLOv8\\datasets\\coco8.zip to D:\\HighwayCounter_YOLOv8\\datasets\\coco8...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 1831.89file/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset download success  (1.4s), saved to \u001b[1mD:\\HighwayCounter_YOLOv8\\datasets\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\i5\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:01<00:00, 523kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3,157,200 parameters, 3,157,184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\engine\\trainer.py:271: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\HighwayCounter_YOLOv8\\datasets\\coco8\\labels\\train... 4 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 25.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\HighwayCounter_YOLOv8\\datasets\\coco8\\labels\\train.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\HighwayCounter_YOLOv8\\datasets\\coco8\\labels\\val... 4 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 121.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\HighwayCounter_YOLOv8\\datasets\\coco8\\labels\\val.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/3     0.705G      1.069      3.506      1.513         21        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4         17      0.619      0.877      0.888      0.615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        2/3     0.638G      1.132      2.784       1.44         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4         17      0.598      0.897      0.888      0.623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        3/3     0.638G      1.019      2.122      1.265         20        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4         17       0.57      0.833      0.874      0.621\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3 epochs completed in 0.003 hours.\n",
            "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.5MB\n",
            "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.5MB\n",
            "\n",
            "Validating runs\\detect\\train\\weights\\best.pt...\n",
            "Ultralytics YOLOv8.2.78  Python-3.8.10 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3060, 12287MiB)\n",
            "Model summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          4         17      0.598      0.898      0.888      0.623\n",
            "                person          3         10      0.644        0.5      0.519      0.286\n",
            "                   dog          1          1      0.319          1      0.995      0.597\n",
            "                 horse          1          2      0.699          1      0.995      0.648\n",
            "              elephant          1          2      0.629      0.887      0.828      0.319\n",
            "              umbrella          1          1      0.541          1      0.995      0.995\n",
            "          potted plant          1          1      0.758          1      0.995      0.895\n",
            "Speed: 1.2ms preprocess, 25.3ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
            "\n",
            "\n",
            "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 105.9ms\n",
            "video 1/1 (frame 2/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 28.0ms\n",
            "video 1/1 (frame 3/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 26.1ms\n",
            "video 1/1 (frame 4/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 30.0ms\n",
            "video 1/1 (frame 5/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 25.0ms\n",
            "video 1/1 (frame 6/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 35.7ms\n",
            "video 1/1 (frame 7/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 23.0ms\n",
            "video 1/1 (frame 8/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 12.9ms\n",
            "video 1/1 (frame 9/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 11.0ms\n",
            "video 1/1 (frame 10/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 13.0ms\n",
            "video 1/1 (frame 11/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 1.1ms\n",
            "video 1/1 (frame 12/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 15.7ms\n",
            "video 1/1 (frame 13/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 13.0ms\n",
            "video 1/1 (frame 14/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 27.3ms\n",
            "video 1/1 (frame 15/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 14.0ms\n",
            "video 1/1 (frame 16/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 4.9ms\n",
            "video 1/1 (frame 17/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 19.8ms\n",
            "video 1/1 (frame 18/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 19/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 34.6ms\n",
            "video 1/1 (frame 20/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 22.9ms\n",
            "video 1/1 (frame 21/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 23.7ms\n",
            "video 1/1 (frame 22/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 11.0ms\n",
            "video 1/1 (frame 23/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 19.3ms\n",
            "video 1/1 (frame 24/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 11.0ms\n",
            "video 1/1 (frame 25/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 24.8ms\n",
            "video 1/1 (frame 26/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 27/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 12.1ms\n",
            "video 1/1 (frame 28/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 5.1ms\n",
            "video 1/1 (frame 29/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 12.0ms\n",
            "video 1/1 (frame 30/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 13.0ms\n",
            "video 1/1 (frame 31/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 12.0ms\n",
            "video 1/1 (frame 32/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 33/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 20.7ms\n",
            "video 1/1 (frame 34/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 35/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 12.2ms\n",
            "video 1/1 (frame 36/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 21.4ms\n",
            "video 1/1 (frame 37/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 13.7ms\n",
            "video 1/1 (frame 38/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 21.7ms\n",
            "video 1/1 (frame 39/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 17.8ms\n",
            "video 1/1 (frame 40/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 18.0ms\n",
            "video 1/1 (frame 41/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 14.0ms\n",
            "video 1/1 (frame 42/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 21.8ms\n",
            "video 1/1 (frame 43/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 11.1ms\n",
            "video 1/1 (frame 44/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 laptop, 11.0ms\n",
            "video 1/1 (frame 45/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 cell phone, 12.6ms\n",
            "video 1/1 (frame 46/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 47/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 14.3ms\n",
            "video 1/1 (frame 48/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 cell phone, 16.2ms\n",
            "video 1/1 (frame 49/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 cell phone, 8.9ms\n",
            "video 1/1 (frame 50/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 12.0ms\n",
            "video 1/1 (frame 51/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 13.0ms\n",
            "video 1/1 (frame 52/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 12.0ms\n",
            "video 1/1 (frame 53/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.1ms\n",
            "video 1/1 (frame 54/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 0.0ms\n",
            "video 1/1 (frame 55/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.0ms\n",
            "video 1/1 (frame 56/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 57/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 58/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.1ms\n",
            "video 1/1 (frame 59/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.8ms\n",
            "video 1/1 (frame 60/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.6ms\n",
            "video 1/1 (frame 61/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 bus, 0.0ms\n",
            "video 1/1 (frame 62/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 bus, 17.9ms\n",
            "video 1/1 (frame 63/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 12.0ms\n",
            "video 1/1 (frame 64/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 26.4ms\n",
            "video 1/1 (frame 65/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 22.3ms\n",
            "video 1/1 (frame 66/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 bus, 14.2ms\n",
            "video 1/1 (frame 67/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 12.7ms\n",
            "video 1/1 (frame 68/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 bus, 16.7ms\n",
            "video 1/1 (frame 69/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.0ms\n",
            "video 1/1 (frame 70/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 bus, 1 truck, 20.8ms\n",
            "video 1/1 (frame 71/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 bus, 1 truck, 9.3ms\n",
            "video 1/1 (frame 72/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 8.6ms\n",
            "video 1/1 (frame 73/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 bus, 13.0ms\n",
            "video 1/1 (frame 74/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.0ms\n",
            "video 1/1 (frame 75/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 76/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 5.1ms\n",
            "video 1/1 (frame 77/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 19.2ms\n",
            "video 1/1 (frame 78/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 13.6ms\n",
            "video 1/1 (frame 79/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 10.2ms\n",
            "video 1/1 (frame 80/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.4ms\n",
            "video 1/1 (frame 81/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 10.1ms\n",
            "video 1/1 (frame 82/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.0ms\n",
            "video 1/1 (frame 83/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1.7ms\n",
            "video 1/1 (frame 84/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 40.1ms\n",
            "video 1/1 (frame 85/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.1ms\n",
            "video 1/1 (frame 86/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 25.8ms\n",
            "video 1/1 (frame 87/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.0ms\n",
            "video 1/1 (frame 88/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 15.1ms\n",
            "video 1/1 (frame 89/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 16.2ms\n",
            "video 1/1 (frame 90/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 25.9ms\n",
            "video 1/1 (frame 91/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.0ms\n",
            "video 1/1 (frame 92/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 93/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 22.1ms\n",
            "video 1/1 (frame 94/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 15.8ms\n",
            "video 1/1 (frame 95/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.1ms\n",
            "video 1/1 (frame 96/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.0ms\n",
            "video 1/1 (frame 97/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 98/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.4ms\n",
            "video 1/1 (frame 99/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.4ms\n",
            "video 1/1 (frame 100/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.0ms\n",
            "video 1/1 (frame 101/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 23.2ms\n",
            "video 1/1 (frame 102/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 15.8ms\n",
            "video 1/1 (frame 103/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 6.7ms\n",
            "video 1/1 (frame 104/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 17.9ms\n",
            "video 1/1 (frame 105/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.0ms\n",
            "video 1/1 (frame 106/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 4.0ms\n",
            "video 1/1 (frame 107/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 18.0ms\n",
            "video 1/1 (frame 108/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 15.0ms\n",
            "video 1/1 (frame 109/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 21.1ms\n",
            "video 1/1 (frame 110/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 13.8ms\n",
            "video 1/1 (frame 111/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 112/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 20.6ms\n",
            "video 1/1 (frame 113/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 16.6ms\n",
            "video 1/1 (frame 114/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 18.9ms\n",
            "video 1/1 (frame 115/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 22.7ms\n",
            "video 1/1 (frame 116/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 skateboard, 8.5ms\n",
            "video 1/1 (frame 117/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 skateboard, 16.7ms\n",
            "video 1/1 (frame 118/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 16.0ms\n",
            "video 1/1 (frame 119/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 15.8ms\n",
            "video 1/1 (frame 120/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 9.9ms\n",
            "video 1/1 (frame 121/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 12.0ms\n",
            "video 1/1 (frame 122/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 0.0ms\n",
            "video 1/1 (frame 123/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 124/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 18.1ms\n",
            "video 1/1 (frame 125/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 8.0ms\n",
            "video 1/1 (frame 126/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 22.3ms\n",
            "video 1/1 (frame 127/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 30.1ms\n",
            "video 1/1 (frame 128/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 54.9ms\n",
            "video 1/1 (frame 129/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 17.0ms\n",
            "video 1/1 (frame 130/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 20.0ms\n",
            "video 1/1 (frame 131/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 21.0ms\n",
            "video 1/1 (frame 132/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 15.0ms\n",
            "video 1/1 (frame 133/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 19.0ms\n",
            "video 1/1 (frame 134/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 12.7ms\n",
            "video 1/1 (frame 135/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 16.7ms\n",
            "video 1/1 (frame 136/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 17.5ms\n",
            "video 1/1 (frame 137/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 11.2ms\n",
            "video 1/1 (frame 138/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 4.5ms\n",
            "video 1/1 (frame 139/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 22.3ms\n",
            "video 1/1 (frame 140/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 18.0ms\n",
            "video 1/1 (frame 141/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 11.0ms\n",
            "video 1/1 (frame 142/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 13.0ms\n",
            "video 1/1 (frame 143/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 22.3ms\n",
            "video 1/1 (frame 144/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 motorcycle, 2.4ms\n",
            "video 1/1 (frame 145/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 146/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 14.0ms\n",
            "video 1/1 (frame 147/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 148/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 12.9ms\n",
            "video 1/1 (frame 149/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 16.0ms\n",
            "video 1/1 (frame 150/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 19.4ms\n",
            "video 1/1 (frame 151/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 16.5ms\n",
            "video 1/1 (frame 152/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 2.4ms\n",
            "video 1/1 (frame 153/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 10.5ms\n",
            "video 1/1 (frame 154/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 0.0ms\n",
            "video 1/1 (frame 155/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 13.0ms\n",
            "video 1/1 (frame 156/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 14.0ms\n",
            "video 1/1 (frame 157/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 16.1ms\n",
            "video 1/1 (frame 158/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 2.8ms\n",
            "video 1/1 (frame 159/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 13.0ms\n",
            "video 1/1 (frame 160/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 8.8ms\n",
            "video 1/1 (frame 161/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 13.1ms\n",
            "video 1/1 (frame 162/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 3 cars, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 163/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 13.0ms\n",
            "video 1/1 (frame 164/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 20.7ms\n",
            "video 1/1 (frame 165/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 6.4ms\n",
            "video 1/1 (frame 166/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 14.6ms\n",
            "video 1/1 (frame 167/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 9.7ms\n",
            "video 1/1 (frame 168/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 5.2ms\n",
            "video 1/1 (frame 169/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 persons, 1 car, 1 motorcycle, 11.0ms\n",
            "video 1/1 (frame 170/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 22.1ms\n",
            "video 1/1 (frame 171/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 17.8ms\n",
            "video 1/1 (frame 172/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 7.3ms\n",
            "video 1/1 (frame 173/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 13.0ms\n",
            "video 1/1 (frame 174/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 11.7ms\n",
            "video 1/1 (frame 175/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 3.8ms\n",
            "video 1/1 (frame 176/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 21.6ms\n",
            "video 1/1 (frame 177/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 178/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 13.0ms\n",
            "video 1/1 (frame 179/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 14.0ms\n",
            "video 1/1 (frame 180/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 12.1ms\n",
            "video 1/1 (frame 181/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 persons, 2 cars, 2 motorcycles, 13.6ms\n",
            "video 1/1 (frame 182/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 10.7ms\n",
            "video 1/1 (frame 183/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 7.0ms\n",
            "video 1/1 (frame 184/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 14.0ms\n",
            "video 1/1 (frame 185/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 2 motorcycles, 5.4ms\n",
            "video 1/1 (frame 186/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 11.9ms\n",
            "video 1/1 (frame 187/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 19.4ms\n",
            "video 1/1 (frame 188/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 12.9ms\n",
            "video 1/1 (frame 189/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 13.0ms\n",
            "video 1/1 (frame 190/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 11.0ms\n",
            "video 1/1 (frame 191/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 24.9ms\n",
            "video 1/1 (frame 192/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 persons, 2 cars, 2 motorcycles, 12.0ms\n",
            "video 1/1 (frame 193/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 2 motorcycles, 16.4ms\n",
            "video 1/1 (frame 194/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 13.6ms\n",
            "video 1/1 (frame 195/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 196/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 17.0ms\n",
            "video 1/1 (frame 197/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 198/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 persons, 2 cars, 1 motorcycle, 13.9ms\n",
            "video 1/1 (frame 199/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 12.6ms\n",
            "video 1/1 (frame 200/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 2 motorcycles, 16.3ms\n",
            "video 1/1 (frame 201/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 3 cars, 1 motorcycle, 30.7ms\n",
            "video 1/1 (frame 202/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 3 cars, 1 motorcycle, 20.5ms\n",
            "video 1/1 (frame 203/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 204/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 205/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 14.8ms\n",
            "video 1/1 (frame 206/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 3 cars, 1 motorcycle, 3.8ms\n",
            "video 1/1 (frame 207/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 3 cars, 1 motorcycle, 13.6ms\n",
            "video 1/1 (frame 208/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 4 cars, 2 motorcycles, 16.3ms\n",
            "video 1/1 (frame 209/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 3 cars, 1 motorcycle, 0.0ms\n",
            "video 1/1 (frame 210/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 4 cars, 1 motorcycle, 22.5ms\n",
            "video 1/1 (frame 211/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 14.0ms\n",
            "video 1/1 (frame 212/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 10.1ms\n",
            "video 1/1 (frame 213/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 5.6ms\n",
            "video 1/1 (frame 214/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 12.0ms\n",
            "video 1/1 (frame 215/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 13.0ms\n",
            "video 1/1 (frame 216/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 16.2ms\n",
            "video 1/1 (frame 217/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 9.1ms\n",
            "video 1/1 (frame 218/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 1 traffic light, 13.9ms\n",
            "video 1/1 (frame 219/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 motorcycle, 1 truck, 1 traffic light, 11.0ms\n",
            "video 1/1 (frame 220/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 traffic light, 22.9ms\n",
            "video 1/1 (frame 221/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 traffic light, 13.0ms\n",
            "video 1/1 (frame 222/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 truck, 1 traffic light, 13.0ms\n",
            "video 1/1 (frame 223/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 traffic light, 15.0ms\n",
            "video 1/1 (frame 224/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 traffic light, 12.7ms\n",
            "video 1/1 (frame 225/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 traffic light, 12.0ms\n",
            "video 1/1 (frame 226/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 traffic light, 12.5ms\n",
            "video 1/1 (frame 227/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 16.7ms\n",
            "video 1/1 (frame 228/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 11.6ms\n",
            "video 1/1 (frame 229/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 13.0ms\n",
            "video 1/1 (frame 230/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 15.0ms\n",
            "video 1/1 (frame 231/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 18.0ms\n",
            "video 1/1 (frame 232/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 13.0ms\n",
            "video 1/1 (frame 233/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 12.0ms\n",
            "video 1/1 (frame 234/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 22.8ms\n",
            "video 1/1 (frame 235/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 19.0ms\n",
            "video 1/1 (frame 236/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 16.5ms\n",
            "video 1/1 (frame 237/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 17.0ms\n",
            "video 1/1 (frame 238/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 17.0ms\n",
            "video 1/1 (frame 239/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 14.3ms\n",
            "video 1/1 (frame 240/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 14.0ms\n",
            "video 1/1 (frame 241/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 0.0ms\n",
            "video 1/1 (frame 242/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 14.5ms\n",
            "video 1/1 (frame 243/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 19.0ms\n",
            "video 1/1 (frame 244/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 16.0ms\n",
            "video 1/1 (frame 245/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 17.0ms\n",
            "video 1/1 (frame 246/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 17.0ms\n",
            "video 1/1 (frame 247/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 6.0ms\n",
            "video 1/1 (frame 248/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 suitcase, 23.5ms\n",
            "video 1/1 (frame 249/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 suitcase, 15.9ms\n",
            "video 1/1 (frame 250/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 2 cars, 1 suitcase, 15.1ms\n",
            "video 1/1 (frame 251/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 suitcase, 16.8ms\n",
            "video 1/1 (frame 252/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 suitcase, 16.2ms\n",
            "video 1/1 (frame 253/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 suitcase, 15.9ms\n",
            "video 1/1 (frame 254/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.9ms\n",
            "video 1/1 (frame 255/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 suitcase, 0.9ms\n",
            "video 1/1 (frame 256/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 suitcase, 12.3ms\n",
            "video 1/1 (frame 257/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 19.3ms\n",
            "video 1/1 (frame 258/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 27.5ms\n",
            "video 1/1 (frame 259/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.7ms\n",
            "video 1/1 (frame 260/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 19.1ms\n",
            "video 1/1 (frame 261/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.0ms\n",
            "video 1/1 (frame 262/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 18.1ms\n",
            "video 1/1 (frame 263/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 14.6ms\n",
            "video 1/1 (frame 264/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 11.0ms\n",
            "video 1/1 (frame 265/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 5.2ms\n",
            "video 1/1 (frame 266/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 11.0ms\n",
            "video 1/1 (frame 267/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 28.9ms\n",
            "video 1/1 (frame 268/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 16.7ms\n",
            "video 1/1 (frame 269/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 1 truck, 16.3ms\n",
            "video 1/1 (frame 270/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.0ms\n",
            "video 1/1 (frame 271/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.0ms\n",
            "video 1/1 (frame 272/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.7ms\n",
            "video 1/1 (frame 273/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 5.6ms\n",
            "video 1/1 (frame 274/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.6ms\n",
            "video 1/1 (frame 275/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.4ms\n",
            "video 1/1 (frame 276/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 16.7ms\n",
            "video 1/1 (frame 277/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.9ms\n",
            "video 1/1 (frame 278/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.1ms\n",
            "video 1/1 (frame 279/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 18.6ms\n",
            "video 1/1 (frame 280/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 15.8ms\n",
            "video 1/1 (frame 281/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.1ms\n",
            "video 1/1 (frame 282/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.8ms\n",
            "video 1/1 (frame 283/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.6ms\n",
            "video 1/1 (frame 284/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.7ms\n",
            "video 1/1 (frame 285/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.6ms\n",
            "video 1/1 (frame 286/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 287/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 16.0ms\n",
            "video 1/1 (frame 288/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 18.0ms\n",
            "video 1/1 (frame 289/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.5ms\n",
            "video 1/1 (frame 290/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.8ms\n",
            "video 1/1 (frame 291/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 16.3ms\n",
            "video 1/1 (frame 292/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.0ms\n",
            "video 1/1 (frame 293/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 3.0ms\n",
            "video 1/1 (frame 294/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 19.2ms\n",
            "video 1/1 (frame 295/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 296/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 9.7ms\n",
            "video 1/1 (frame 297/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 4.7ms\n",
            "video 1/1 (frame 298/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 18.3ms\n",
            "video 1/1 (frame 299/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.0ms\n",
            "video 1/1 (frame 300/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 16.1ms\n",
            "video 1/1 (frame 301/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.6ms\n",
            "video 1/1 (frame 302/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 16.4ms\n",
            "video 1/1 (frame 303/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 304/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 6.3ms\n",
            "video 1/1 (frame 305/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 17.1ms\n",
            "video 1/1 (frame 306/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 307/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 7.3ms\n",
            "video 1/1 (frame 308/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 17.7ms\n",
            "video 1/1 (frame 309/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 9.9ms\n",
            "video 1/1 (frame 310/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.1ms\n",
            "video 1/1 (frame 311/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 312/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 19.0ms\n",
            "video 1/1 (frame 313/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.0ms\n",
            "video 1/1 (frame 314/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 315/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 4.8ms\n",
            "video 1/1 (frame 316/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 12.0ms\n",
            "video 1/1 (frame 317/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 21.6ms\n",
            "video 1/1 (frame 318/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 0.0ms\n",
            "video 1/1 (frame 319/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.0ms\n",
            "video 1/1 (frame 320/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 9.2ms\n",
            "video 1/1 (frame 321/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 15.0ms\n",
            "video 1/1 (frame 322/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.0ms\n",
            "video 1/1 (frame 323/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 19.0ms\n",
            "video 1/1 (frame 324/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.5ms\n",
            "video 1/1 (frame 325/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 14.2ms\n",
            "video 1/1 (frame 326/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.8ms\n",
            "video 1/1 (frame 327/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 15.5ms\n",
            "video 1/1 (frame 328/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 23.4ms\n",
            "video 1/1 (frame 329/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 17.7ms\n",
            "video 1/1 (frame 330/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 25.4ms\n",
            "video 1/1 (frame 331/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 3.7ms\n",
            "video 1/1 (frame 332/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 9.3ms\n",
            "video 1/1 (frame 333/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 0.0ms\n",
            "video 1/1 (frame 334/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 21.6ms\n",
            "video 1/1 (frame 335/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 16.5ms\n",
            "video 1/1 (frame 336/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 6.0ms\n",
            "video 1/1 (frame 337/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 25.2ms\n",
            "video 1/1 (frame 338/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 5.1ms\n",
            "video 1/1 (frame 339/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 11.0ms\n",
            "video 1/1 (frame 340/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 13.8ms\n",
            "video 1/1 (frame 341/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 12.8ms\n",
            "video 1/1 (frame 342/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 9.6ms\n",
            "video 1/1 (frame 343/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 27.0ms\n",
            "video 1/1 (frame 344/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 car, 24.5ms\n",
            "video 1/1 (frame 345/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 13.0ms\n",
            "video 1/1 (frame 346/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 (no detections), 18.0ms\n",
            "video 1/1 (frame 347/29116) d:\\HighwayCounter_YOLOv8\\test_video.mp4: 384x640 1 suitcase, 43.0ms\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# load a pretrained YOLOv8n detection model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoco8.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_video.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# predict on an image\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\engine\\model.py:173\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    146\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\engine\\model.py:564\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 564\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\_contextlib.py:57\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 57\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\engine\\predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:456\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 456\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\nn\\tasks.py:102\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\nn\\tasks.py:120\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\nn\\tasks.py:141\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    142\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:240\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    239\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\i5\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\n",
        "model.train(data='coco8.yaml', epochs=3)  # train the model\n",
        "model('test_video.mp4')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZW58jUzK66B"
      },
      "source": [
        "## 2. Segmentation\n",
        "\n",
        "YOLOv8 _segmentation_ models use the `-seg` suffix, i.e. `yolov8n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFPJIQl_L5HT"
      },
      "outputs": [],
      "source": [
        "# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-seg.pt')  # load a pretrained YOLOv8n segmentation model\n",
        "model.train(data='coco8-seg.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax3p94VNK9zR"
      },
      "source": [
        "## 3. Classification\n",
        "\n",
        "YOLOv8 _classification_ models use the `-cls` suffix, i.e. `yolov8n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q9Zu6zlL5rS"
      },
      "outputs": [],
      "source": [
        "# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n",
        "model.train(data='mnist160', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpIaFLiO11TG"
      },
      "source": [
        "## 4. Pose\n",
        "\n",
        "YOLOv8 _pose_ models use the `-pose` suffix, i.e. `yolov8n-pose.pt` and are pretrained on COCO Keypoints. See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for full details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si4aKFNg19vX"
      },
      "outputs": [],
      "source": [
        "# Load YOLOv8n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n pose model\n",
        "model.train(data='coco8-pose.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf5j_T9-B5F0"
      },
      "source": [
        "## 4. Oriented Bounding Boxes (OBB)\n",
        "\n",
        "YOLOv8 _OBB_ models use the `-obb` suffix, i.e. `yolov8n-obb.pt` and are pretrained on the DOTA dataset. See [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for full details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJNKClOOB5YS"
      },
      "outputs": [],
      "source": [
        "# Load YOLOv8n-obb, train it on DOTA8 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-obb.pt')  # load a pretrained YOLOv8n OBB model\n",
        "model.train(data='coco8-dota.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIdE6i8C3LYp"
      },
      "outputs": [],
      "source": [
        "# Pip install from source\n",
        "!pip install git+https://github.com/ultralytics/ultralytics@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRKlwxSJdhd1"
      },
      "outputs": [],
      "source": [
        "# Git clone and run tests on updates branch\n",
        "!git clone https://github.com/ultralytics/ultralytics -b main\n",
        "%pip install -qe ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtPlh7mcCGZX"
      },
      "outputs": [],
      "source": [
        "# Run tests (Git clone only)\n",
        "!pytest ultralytics/tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdc6t_bfzDDk"
      },
      "outputs": [],
      "source": [
        "# Validate multiple models\n",
        "for x in 'nsmlx':\n",
        "  !yolo val model=yolov8{x}.pt data=coco.yaml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YOLOv8 Tutorial",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
